{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9WuvonxZOcT"
   },
   "source": [
    "# W7 - Statistics in Python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuzzqbpbYtZg"
   },
   "source": [
    "## iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "djtBNWpeY1UN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset into a DataFrame\n",
    "# Replace 'your_dataset.csv' with your actual file path or URL\n",
    "food_consumption = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Calculate total co2_emission per country: emissions_by_country\n",
    "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
    "\n",
    "# Compute the first and third quartiles (25th and 75th percentiles) and IQR of emissions_by_country\n",
    "q1 = np.percentile(emissions_by_country, 25)\n",
    "q3 = np.percentile(emissions_by_country, 75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Calculate the lower and upper cutoffs for outliers\n",
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr\n",
    "\n",
    "# Subset emissions_by_country to find outliers\n",
    "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\n",
    "\n",
    "# Print the outliers\n",
    "print(\"Outliers:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35MFPENwZVHo"
   },
   "source": [
    "## Calculating probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqEyaumJZX3d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset into a DataFrame\n",
    "# Replace 'amir_deals.csv' with your actual file path or URL\n",
    "amir_deals = pd.read_csv('amir_deals.csv')\n",
    "\n",
    "# Count the deals for each product\n",
    "counts = amir_deals['product'].value_counts()\n",
    "\n",
    "# Calculate the probability of picking a deal with each product\n",
    "probs = counts / amir_deals.shape[0]\n",
    "\n",
    "# Print the probabilities\n",
    "print(\"Probabilities of picking a deal for each product:\")\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doS5GGugZkmA"
   },
   "source": [
    "## probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flpBA2EnZkIE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset into a DataFrame\n",
    "# Replace 'restaurant_groups.csv' with your actual file path or URL\n",
    "restaurant_groups = pd.read_csv('restaurant_groups.csv')\n",
    "\n",
    "# Plot a histogram of group sizes\n",
    "restaurant_groups['group_size'].hist(bins=[2, 3, 4, 5, 6])\n",
    "plt.show()\n",
    "\n",
    "# Create probability distribution\n",
    "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
    "\n",
    "# Reset index and rename columns\n",
    "size_dist = size_dist.reset_index()\n",
    "size_dist.columns = ['group_size', 'prob']\n",
    "\n",
    "# Print the probability distribution\n",
    "print(\"Probability distribution of group sizes:\")\n",
    "print(size_dist)\n",
    "\n",
    "# Calculate the expected value\n",
    "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
    "print('The expected value is', expected_value)\n",
    "\n",
    "# Subset groups of size 4 or more\n",
    "groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n",
    "\n",
    "# Sum the probabilities of groups_4_or_more\n",
    "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
    "print('The probability of groups with 4 or more people is', prob_4_or_more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_C-MbOJaQXv"
   },
   "source": [
    "## calculate probabilities and generate random wait times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rOc9QXlaRej"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "\n",
    "min_time = 0\n",
    "max_time = 30\n",
    "\n",
    "# Calculate probability of waiting more than 5 mins\n",
    "prob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\n",
    "print(\"Probability of waiting more than 5 mins:\", prob_greater_than_5)\n",
    "\n",
    "# Calculate probability of waiting 10-20 mins\n",
    "prob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)\n",
    "print(\"Probability of waiting between 10 and 20 mins:\", prob_between_10_and_20)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(334)\n",
    "\n",
    "# Generate 1000 wait times between 0 and 30 mins\n",
    "wait_times = uniform.rvs(min_time, max_time, size=1000)\n",
    "\n",
    "# Create a histogram of simulated times and show the plot\n",
    "plt.hist(wait_times, bins=20)  # You can adjust the number of bins as needed\n",
    "plt.xlabel(\"Wait Time (mins)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Simulated Wait Times\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEWeF7Z9az5v"
   },
   "source": [
    "##generating sample means\n",
    "from a dataset and creating a histogram using a sample size of 20 and a loop of 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUDXn6w9a0aS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(104)\n",
    "\n",
    "sample_means = []\n",
    "\n",
    "# Loop 100 times\n",
    "for i in range(100):\n",
    "    # Take a sample of 20 num_users with replacement\n",
    "    samp_20 = amir_deals['num_users'].sample(20, replace=True)\n",
    "\n",
    "    # Calculate the mean of samp_20\n",
    "    samp_20_mean = np.mean(samp_20)\n",
    "\n",
    "    # Append samp_20_mean to sample_means\n",
    "    sample_means.append(samp_20_mean)\n",
    "\n",
    "# Convert to a Series and plot the histogram\n",
    "sample_means_series = pd.Series(sample_means)\n",
    "sample_means_series.hist(bins=15)  # You can adjust the number of bins as needed\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Sample Means\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Sample Means\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPQxfil6bUiU"
   },
   "source": [
    "## creating a scatterplot, adding a trendline and calculating the correlation between two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jeLKdfbU_F"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset into a DataFrame\n",
    "# Replace 'data=world_happiness' with your actual file path or URL\n",
    "world_happiness = pd.read_csv('world_happiness.csv')\n",
    "# Create a scatterplot of happiness_score vs. life_exp\n",
    "sns.scatterplot(x='life_exp', y='happiness_score', data=world_happiness)\n",
    "plt.title(\"Scatterplot of Happiness Score vs. Life Expectancy\")\n",
    "plt.xlabel(\"Life Expectancy\")\n",
    "plt.ylabel(\"Happiness Score\")\n",
    "plt.show()\n",
    "\n",
    "# Create a scatterplot of happiness_score vs. life_exp with a trendline\n",
    "sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)\n",
    "plt.title(\"Scatterplot with Trendline of Happiness Score vs. Life Expectancy\")\n",
    "plt.xlabel(\"Life Expectancy\")\n",
    "plt.ylabel(\"Happiness Score\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the correlation between life_exp and happiness_score\n",
    "correlation = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n",
    "print(\"Correlation between Life Expectancy and Happiness Score:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXI3Ut_7WU7G"
   },
   "source": [
    "# W8 - Regression with Statsmodels (1).pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_azqT7MPY5UC"
   },
   "source": [
    "## all from chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDYuM0U1WVUE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Taiwan real estate dataset\n",
    "taiwan_real_estate = pd.read_csv('Taiwan_real_estate2.csv')\n",
    "\n",
    "# Visualizing two numeric variables\n",
    "sns.regplot(x=\"price_twd_msq\", y=\"n_convenience\", data=taiwan_real_estate, scatter_kws={'alpha': 0.5}, ci=None)\n",
    "plt.show()\n",
    "\n",
    "# Linear regression with ols()\n",
    "mdl_price_vs_conv = ols('price_twd_msq ~ n_convenience', data=taiwan_real_estate).fit()\n",
    "print(mdl_price_vs_conv.params)\n",
    "\n",
    "# Predicting values\n",
    "explanatory_data = pd.DataFrame({'n_convenience': np.arange(0, 11)})\n",
    "price_twd_msq = mdl_price_vs_conv.predict(explanatory_data)\n",
    "prediction_data = explanatory_data.assign(price_twd_msq=price_twd_msq)\n",
    "print(prediction_data)\n",
    "\n",
    "# Manually predicting values\n",
    "coeffs = mdl_price_vs_conv.params\n",
    "intercept = coeffs[0]\n",
    "slope = coeffs[1]\n",
    "price_twd_msq = intercept + slope * explanatory_data['n_convenience']\n",
    "print(price_twd_msq)\n",
    "print(price_twd_msq.assign(predictions_auto=mdl_price_vs_conv.predict(explanatory_data)))\n",
    "\n",
    "# Transforming variables\n",
    "taiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n",
    "\n",
    "sns.regplot(x=\"dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n",
    "plt.show()\n",
    "\n",
    "mdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n",
    "print(mdl_price_vs_dist.params)\n",
    "\n",
    "explanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2), \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n",
    "prediction_data = explanatory_data.assign(price_twd_msq=mdl_price_vs_dist.predict(explanatory_data))\n",
    "print(prediction_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.regplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n",
    "sns.scatterplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=prediction_data, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "# Quantifying Model Fit\n",
    "mse = mdl_bream.mse_resid\n",
    "print('mse: ', mse)\n",
    "\n",
    "rse = np.sqrt(mse)\n",
    "print(\"rse: \", rse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZC8v86fZCBo"
   },
   "source": [
    "##  linear regression model using the ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZBDXHTzZEiq"
   },
   "outputs": [],
   "source": [
    "# Import the ols function\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Load your dataset into a DataFrame\n",
    "# Replace 'taiwan_real_estate2.csv' with your actual file path or URL\n",
    "taiwan_real_estate = pd.read_csv('taiwan_real_estate2.csv')\n",
    "\n",
    "# Create the model object\n",
    "mdl_price_vs_conv = ols('price_twd_msq ~ n_convenience', data=taiwan_real_estate)\n",
    "\n",
    "# Fit the model\n",
    "mdl_price_vs_conv = mdl_price_vs_conv.fit()\n",
    "\n",
    "# Print the parameters of the fitted model\n",
    "print(mdl_price_vs_conv.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV-KXoYhc0YO"
   },
   "source": [
    "## transformed variables and linear regressio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "rcU6QQcOczl_",
    "outputId": "8824e286-f363-44d1-c6f9-245cb5ee6cba"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-02a4b8cf9870>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load your dataset into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Replace 'taiwan_real_estate2.csv' with your actual file path or URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtaiwan_real_estate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taiwan_real_estate2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create a scatter plot of 'dist_to_mrt_m' vs. 'price_twd_msq'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'taiwan_real_estate2.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Load your dataset into a DataFrame\n",
    "# Replace 'taiwan_real_estate2.csv' with your actual file path or URL\n",
    "taiwan_real_estate = pd.read_csv('taiwan_real_estate2.csv')\n",
    "\n",
    "# Create a scatter plot of 'dist_to_mrt_m' vs. 'price_twd_msq'\n",
    "sns.regplot(x=\"dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n",
    "plt.show()\n",
    "\n",
    "# Create a new column 'sqrt_dist_to_mrt_m' by taking the square root of 'dist_to_mrt_m'\n",
    "taiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n",
    "\n",
    "# Create a scatter plot using the transformed variable 'sqrt_dist_to_mrt_m' vs. 'price_twd_msq'\n",
    "plt.figure()\n",
    "sns.regplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n",
    "plt.show()\n",
    "\n",
    "# Run a linear regression of 'price_twd_msq' vs. the square root of 'dist_to_mrt_m' using taiwan_real_estate\n",
    "mdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n",
    "\n",
    "# Print the parameters of the fitted model\n",
    "print(mdl_price_vs_dist.params)\n",
    "\n",
    "# Create explanatory_data with different values of 'sqrt_dist_to_mrt_m'\n",
    "explanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2), \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n",
    "\n",
    "# Create prediction_data by adding a column of predictions to explanatory_data\n",
    "prediction_data = explanatory_data.assign(price_twd_msq=mdl_price_vs_dist.predict(explanatory_data))\n",
    "\n",
    "# Create a scatter plot of 'sqrt_dist_to_mrt_m' vs. 'price_twd_msq' with the fitted trendline and prediction points\n",
    "fig = plt.figure()\n",
    "sns.regplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n",
    "sns.scatterplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=prediction_data, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "# Create another scatter plot of 'dist_to_mrt_m' vs. 'price_twd_msq' with the prediction points\n",
    "fig = plt.figure()\n",
    "sns.regplot(x=\"dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n",
    "sns.scatterplot(x=\"dist_to_mrt_m\", y=\"price_twd_msq\", data=prediction_data, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5nMoYa3VWlF"
   },
   "source": [
    "# W9 - Machine Learning with Scikit-learn (1).pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s1w39nhpCQs"
   },
   "source": [
    "## all from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOOK02TpVovg"
   },
   "outputs": [],
   "source": [
    "# Machine Learning with Scikit-learn\n",
    "\n",
    "# Supervised Machine Learning\n",
    "\n",
    "# What is machine learning?\n",
    "# Machine learning is the process whereby:\n",
    "# Computers are given the ability to learn to make decisions from data without being explicitly programmed.\n",
    "\n",
    "# Unsupervised learning\n",
    "# Uncovering hidden patterns from unlabeled data\n",
    "# Example:\n",
    "# Grouping customers into distinct categories (Clustering) Cluster Analysis for Customer Churn\n",
    "\n",
    "# Supervised learning\n",
    "# The predicted values are known\n",
    "# Aim: Predict the target values of unseen data, given the features\n",
    "\n",
    "# Features\n",
    "# Target variable points_per_game assists_per_game rebounds_per_game steals_per_game blocks_per_game position\n",
    "# 26.9 6.6 4.5 11 0.4 Point Guard\n",
    "# 13 1.7 4 0.4 1.3 Center\n",
    "# 17.6 2.3 7.9 1.0 0.8 Power Forward\n",
    "# 22.6 4.5 4.4 1.2 0.4 Shooting Guard\n",
    "\n",
    "# Types of supervised learning\n",
    "# Classification: Target variable consists of categories\n",
    "# Regression: Target variable is continuous\n",
    "\n",
    "# Classifying labels of unseen data\n",
    "# 1. Build a model\n",
    "# 2. Model learns from the labeled data we pass to it\n",
    "# 3. Pass unlabeled data to the model as input\n",
    "# 4. Model predicts the labels of the unseen data\n",
    "# Labeled data = training data\n",
    "\n",
    "# k-Nearest Neighbors\n",
    "# Predict the label of a data point by\n",
    "# Looking at the k closest labeled data points\n",
    "# Taking a majority vote\n",
    "\n",
    "# To build intuition for KNN, let's look at this scatter plot displaying total evening charge against total day charge for customers of a telecom company.\n",
    "# The observations are colored in blue for customers who have churned, and red for those who have not churned.\n",
    "# Here we have visualized the results of a KNN algorithm where the number of neighbors is set to 15.\n",
    "# KNN creates a decision boundary to predict if customers will churn.\n",
    "# Any customers in the area with a gray background are predicted to churn, and those in the area with a red background are predicted to not churn.\n",
    "# This boundary would be used to make predictions on unseen data.\n",
    "\n",
    "# Using scikit-learn to fit a classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = churn_df[[\"total_day_charge\", \"total_eve_charge\"]].values\n",
    "y = churn_df[\"churn\"].values\n",
    "print(X.shape, y.shape)\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# 1 corresponds to churn and 0 corresponds to no churn\n",
    "\n",
    "# k-Nearest Neighbors: Fit\n",
    "# In this exercise, you will build your first classification model using the churn_df dataset, can be loaded from churn_df.csv.\n",
    "# The features to use will be \"account_length\" and \"customer_service_calls\".\n",
    "# The target, \"churn\", needs to be a single column with the same number of observations as the feature data.\n",
    "# You will convert the features and the target variable into NumPy arrays, create an instance of a KNN classifier, and then fit it to the data.\n",
    "# Instructions\n",
    "# • Import KNeighborsClassifier from sklearn.neighbors.\n",
    "# • Create an array called X containing values from the \"account_length\" and \"customer_service_calls\" columns, and an array called y for the values of the \"churn\" column.\n",
    "# • Instantiate a KNeighborsClassifier called knn with 6 neighbors.\n",
    "# • Fit the classifier to the data using the .fit() method.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = churn_df[[\"account_length\", \"customer_service_calls\"]].values\n",
    "y = churn_df[\"churn\"].values\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# k-Nearest Neighbors: Predict\n",
    "# Now you have fit a KNN classifier, you can use it to predict the label of new data points.\n",
    "# All available data was used for training, however, fortunately, there are new observations available, X_new.\n",
    "# The model knn, which you created and fit the data in the last exercise, will be used.\n",
    "# You will use your classifier to predict the labels of a set of new data points:\n",
    "X_new = np.array([[30.0, 17.5], [107.0, 24.1], [213.0, 10.9]])\n",
    "y_pred = knn.predict(X_new)\n",
    "print(y_pred)\n",
    "\n",
    "# Measuring model performance\n",
    "# Computing accuracy\n",
    "# Training set\n",
    "# Split data\n",
    "# Fit/train classifier on training set\n",
    "# Calculate accuracy using test set\n",
    "# Test set\n",
    "# It is best practice to ensure our split reflects the proportion of labels in our data.\n",
    "# So if churn occurs in 10% of observations, we want 10% of labels in our training and test sets to represent churn.\n",
    "# We achieve this by setting stratify equal to y.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "# Decision boundaries, underfitting, and overfitting\n",
    "# Recall that we discussed decision boundaries, which are thresholds for determining what label a model assigns to an observation.\n",
    "# In the image shown, as k increases, the decision boundary is less affected by individual observations, reflecting a simpler model.\n",
    "# Simpler models are less able to detect relationships in the dataset, which is known as underfitting.\n",
    "# Complex models can be sensitive to noise in the training data, rather than reflecting general trends. This is known as overfitting.\n",
    "\n",
    "# Variance and bias\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "neighbors = np.arange(1, 26)\n",
    "for neighbor in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracies[neighbor] = knn.score(X_train, y_train)\n",
    "    test_accuracies[neighbor] = knn.score(X_test, y_test)\n",
    "\n",
    "my_train = list(train_accuracies.values())\n",
    "my_test = list(test_accuracies.values())\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('KNN: Varing Number of Neighbors')\n",
    "plt.plot(neighbors, my_train, label='Training Accuracy')\n",
    "plt.plot(neighbors, my_test, label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Here's the result! As k increases beyond 15 we see overfitting where performance plateaus on both test and training sets, as indicated in this plot. Peak occurs at around 16.\n",
    "\n",
    "# Exercise\n",
    "# NumPy arrays have been created for you containing the features as X and the target variable as y. You will split them into training and test sets, fit a KNN classifier to the training data, and then compute its accuracy on the test data using the .score() method.\n",
    "# Instructions:\n",
    "# • Import train_test_split from sklearn.model_selection.\n",
    "# • Create Numpy arrays having all columns, except churn, as features X, and target variable, churn, as target variable y\n",
    "# • Split X and y into training and test sets, setting test_size equal to 20%, random_state to 42, and ensuring the target label proportions reflect that of the original dataset.\n",
    "# • Fit the knn model to the training data.\n",
    "# • Compute and print the model's accuracy for the test data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = churn_df.drop(\"churn\", axis=1).values\n",
    "y = churn_df[\"churn\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "\n",
    "# Regression with Scikit-Learn\n",
    "\n",
    "# Predicting blood glucose levels\n",
    "import pandas as pd\n",
    "diabetes_df = pd.read_csv(\"diabetes.csv\")\n",
    "X_bmi = diabetes_df[\"bmi\"].values.reshape(-1, 1)\n",
    "y = diabetes_df[\"glucose\"].values\n",
    "\n",
    "# Fitting a regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_bmi, y)\n",
    "predictions = reg.predict(X_bmi)\n",
    "\n",
    "plt.scatter(X_bmi, y)\n",
    "plt.plot(X_bmi, predictions)\n",
    "plt.ylabel(\"Blood Glucose (mg/dl)\")\n",
    "plt.xlabel(\"Body Mass Index\")\n",
    "plt.show()\n",
    "\n",
    "# Let’s practice\n",
    "# Creating features\n",
    "# In this exercise, you will work with a dataset called sales_df (loaded from sales_df.csv), which contains information on advertising campaign expenditure across different media types, and the number of dollars generated in sales for the respective campaign.\n",
    "# The dataset has been preloaded for you. Here are the first two rows:\n",
    "# tv 1 13000.0 2 41000.0\n",
    "# radio 9237.76 15886.45\n",
    "# social_media sales 2409.57 46677.90 2913.41 150177.83\n",
    "# You will use the advertising expenditure as features to predict sales values, initially working with the \"radio\" column.\n",
    "# However, before you make any predictions you will need to create the feature and target arrays, reshaping them to the correct format for scikit-learn.\n",
    "# • Create X, an array of the values from the sales_df DataFrame's \"radio\" column.\n",
    "# • Create y, an array of the values from the sales_df DataFrame's \"sales\" column.\n",
    "# • Reshape X into a two-dimensional NumPy array.\n",
    "# • Print the shape of X and y.\n",
    "X = sales_df[\"radio\"].values\n",
    "y = sales_df[\"sales\"].values\n",
    "X = X.reshape(-1, 1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Building a linear regression model\n",
    "# Now you have created your feature and target arrays, you will train a linear regression model on all feature and target values.\n",
    "# As the goal is to assess the relationship between the feature and target values there is no need to split the data into training and test sets.\n",
    "# • Instantiate a linear regression model.\n",
    "# • Predict sales values using X, storing as predictions.\n",
    "# • Print five prediction values.\n",
    "# Visualizing a linear regression model\n",
    "# Now you have built your linear regression model and trained it using all available observations, you can visualize how well the model fits the data. This allows you to interpret the relationship between radio advertising expenditure and sales values.\n",
    "# The variables X, an array of radio values, y, an array of sales values, and predictions, an array of the model's predicted values for y given X, have all been preloaded for you from the previous exercise.\n",
    "# • Create a scatter plot visualizing y against X, with observations in blue.\n",
    "# • Draw a red line plot displaying the predictions against X.\n",
    "# • Display the plot.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "predictions = reg.predict(X)\n",
    "\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, predictions, color='red')\n",
    "plt.xlabel('Radio Advertising Expenditure')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "# The loss function\n",
    "# The distance is called a residual. We could try to minimize the sum of the residuals, each negative residual. To avoid this, we square the residuals. By adding all the squared residuals, we calculate the residual sum of squares, or RSS. This type of linear regression is called Ordinary Least Squares, or OLS, where we aim to minimize the RSS.\n",
    "# Linear regression in higher dimensions y = a1x1 + a2x2 + a3x3 + ... + antn + b\n",
    "# • To fit a linear regression model here:\n",
    "# • Need to specify 3 variables: a1, a2, b\n",
    "# • In higher dimensions:\n",
    "# • Known as multiple regression\n",
    "# • Must specify coefficients for each feature and the variable b\n",
    "# y = a1x1 + a2x2 + a3x3 + ... + antn + b\n",
    "# • scikit-learn works exactly the same way:\n",
    "# • Pass two arrays: features and target\n",
    "\n",
    "# Linear regression using all features\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "# reg_all = LinearRegression()\n",
    "# reg_all.fit(X_train, y_train)\n",
    "# Y_pred = reg_all.predict(X_test)\n",
    "\n",
    "# R-squared\n",
    "# • R2: quantifies the variance in target values explained by the features\n",
    "# • Values range from 0 to 1\n",
    "# • Low R2: poor model fit\n",
    "# • High R2: good model fit\n",
    "# To compute R-squared, we call the model’s .score() method, passing the test features and targets. Here the features only explain about 35 percent of blood glucose level variance.\n",
    "# The model has an average error for blood glucose levels of around 24 milligrams per deciliter.\n",
    "# Fit and predict for regression\n",
    "# Now you have seen how linear regression works, your task is to create a multiple linear regression model using all of the features in the sales_df dataset.\n",
    "# As a reminder, here are the first two rows:\n",
    "# You will then use this model to predict sales based on the values of the test features.\n",
    "# • Create X, an array containing values of all features in sales_df, and y, containing all values from the \"sales\" column.\n",
    "# • Instantiate a linear regression model.\n",
    "# • Fit the model to the data.\n",
    "# • Create y_pred, making predictions for sales using the test features.\n",
    "# • Print the first two values of y_pred and y_test\n",
    "# • Calculate the model's R-squared score by passing the test feature values and the test target values to an appropriate method.\n",
    "# • Calculate the model's root mean squared error using y_test and y_pred.\n",
    "# • Print r_squared and rmse.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = sales_df.drop(\"sales\", axis=1).values\n",
    "y = sales_df[\"sales\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(y_pred[:2])\n",
    "print(y_test[:2])\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "print(\"R-squared:\", reg.score(X_test, y_test))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Cross-validation for R-squared\n",
    "# Cross-validation is a vital approach to evaluating a model. It maximizes the amount of data that is available to the model, as the model is not only trained but also tested on all of the available data.\n",
    "# In this exercise, you will build a linear regression model, then use 6-fold cross-validation to assess its accuracy for predicting sales using social media advertising expenditure. You will display the individual score for each of the six-folds.\n",
    "# The sales_df dataset shall be split into y for the target variable (sales), and X for the features (radio and social media).\n",
    "# Instructions\n",
    "# • Import KFold and cross_val_score.\n",
    "# • Create X and y according to the requirements above.\n",
    "# • Create kf by calling KFold(), setting the number of splits to six, shuffle to True, and setting a seed of 5.\n",
    "# • Perform cross-validation using reg on X and y, passing kf to cv.\n",
    "# • Print the cv_scores.\n",
    "# • Calculate and print the mean of the cv_scores results.\n",
    "# • Calculate and print the standard deviation of cv_scores.\n",
    "# • Display the 95% confidence interval for your results using np.quantile().\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
    "cv_scores = cross_val_score(reg, X, y, cv=kf)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV R-squared:\", np.mean(cv_scores))\n",
    "print(\"CV R-squared std:\", np.std(cv_scores))\n",
    "confidence_interval = np.quantile(cv_scores, [0.025, 0.975])\n",
    "print(\"95% Confidence Interval:\", confidence_interval)\n",
    "\n",
    "# Regularized regression\n",
    "# from sklearn.linear_model import Lasso\n",
    "# diabetes_df = pd.read_csv('diabetes.csv', index_col = 0)\n",
    "# diabetes_df = diabetes_df[diabetes_df['bmi'] != 0]\n",
    "# diabetes_df = diabetes_df[diabetes_df['glucose'] != 0]\n",
    "# X = diabetes_df.drop('glucose', axis=1).values\n",
    "# y = diabetes_df['glucose'].values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# scores = []\n",
    "# for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:\n",
    "#     lasso = Lasso(alpha=alpha)\n",
    "#     lasso.fit(X_train, y_train)\n",
    "#     lasso_pred = lasso.predict(X_test)\n",
    "#     scores.append(lasso.score(X_test, y_test))\n",
    "# print(scores)\n",
    "from sklearn.linear_model import Lasso\n",
    "diabetes_df = pd.read_csv('diabetes.csv', index_col=0)\n",
    "diabetes_df = diabetes_df[diabetes_df['bmi'] != 0]\n",
    "diabetes_df = diabetes_df[diabetes_df['glucose'] != 0]\n",
    "X = diabetes_df.drop('glucose', axis=1).values\n",
    "y = diabetes_df['glucose'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scores = []\n",
    "for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    lasso_pred = lasso.predict(X_test)\n",
    "    scores.append(lasso.score(X_test, y_test))\n",
    "print(scores)\n",
    "\n",
    "# Lasso regression for feature importance\n",
    "# Earlier, you saw how lasso regression can be used to identify important features in a dataset.\n",
    "# In this exercise, you will fit a lasso regression model to the sales_df data and plot the model's coefficients.\n",
    "# • The feature variables (all columns except sales) and target variable (sales) arrays have to be created as X and y, along with sales_columns, which contains the dataset's feature names.\n",
    "# • Instantiate a Lasso regressor with an alpha of 0.1.\n",
    "# • Fit the model to the data.\n",
    "# • Compute the model's coefficients, storing as lasso_coef.\n",
    "from sklearn.linear_model import Lasso\n",
    "names = sales_df.drop(\"sales\", axis=1).columns\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso_coef = lasso.fit(X, y).coef_\n",
    "plt.bar(names, lasso_coef)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Lasso regression for feature importance\n",
    "# Earlier, you saw how lasso regression can be used to identify important features in a dataset.\n",
    "# In this exercise, you will fit a lasso regression model to the sales_df data and plot the model's coefficients.\n",
    "# • The feature variables (all columns except sales) and target variable (sales) arrays have to be created as X and y, along with sales_columns, which contains the dataset's feature names.\n",
    "# • Instantiate a Lasso regressor with an alpha of 0.1.\n",
    "# • Fit the model to the data.\n",
    "# • Compute the model's coefficients, storing as lasso_coef.\n",
    "from sklearn.linear_model import Lasso\n",
    "names = sales_df.drop(\"sales\", axis=1).columns\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso_coef = lasso.fit(X, y).coef_\n",
    "plt.bar(names, lasso_coef)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZuKI5DOnlg2"
   },
   "source": [
    "## the k-Nearest Neighbors (KNN) classifier to predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVYvzGfDnl_3"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load your dataset into a DataFrame\n",
    "# Replace 'taiwan_real_estate2.csv' with your actual file path or URL\n",
    "churn_df = pd.read_csv('churn_df.csv')\n",
    "X = churn_df[[\"account_length\", \"customer_service_calls\"]].values\n",
    "y = churn_df[\"churn\"].values\n",
    "# Assuming you have already prepared your data and loaded it into X and y\n",
    "\n",
    "# Create a KNN classifier with a specified number of neighbors (e.g., 6)\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the KNN classifier to your training data\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Define new data points for prediction\n",
    "X_new = np.array([[30.0, 17.5], [107.0, 24.1], [213.0, 10.9]])\n",
    "\n",
    "# Use the trained KNN classifier to predict the labels for the new data points\n",
    "y_pred = knn.predict(X_new)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2pcJwlPn9hj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load your data into a DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assuming 'X' contains your feature columns and 'y' contains your target column\n",
    "X = data[['feature1', 'feature2', ...]]  # Replace with actual feature names\n",
    "y = data['target_column']  # Replace with actual target column name\n",
    "\n",
    "# Split the data into training and testing sets with a 70-30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "# Create a KNN classifier with a specified number of neighbors (e.g., 2)\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# Fit the KNN classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print the accuracy score on the test data\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Z4kW6RoiSj"
   },
   "source": [
    "## numbers of neighbors, fit K-Nearest Neighbors (KNN) classifiers, and plot the training and testing accuracies as a function of the number of neighbors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogvvVeSCoif8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load your data into a DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assuming 'X' contains your feature columns and 'y' contains your target column\n",
    "X = data[['feature1', 'feature2', ...]]  # Replace with actual feature names\n",
    "y = data['target_column']  # Replace with actual target column name\n",
    "\n",
    "# Split the data into training and testing sets with a 70-30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "# Define a range of neighbors to evaluate\n",
    "neighbors = np.arange(1, 26)\n",
    "\n",
    "# Initialize dictionaries to store training and testing accuracies\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "# Loop through different numbers of neighbors\n",
    "for neighbor in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate and store training accuracy\n",
    "    train_accuracies[neighbor] = knn.score(X_train, y_train)\n",
    "\n",
    "    # Calculate and store testing accuracy\n",
    "    test_accuracies[neighbor] = knn.score(X_test, y_test)\n",
    "\n",
    "# Convert accuracy values to lists for plotting\n",
    "my_train = list(train_accuracies.values())\n",
    "my_test = list(test_accuracies.values())\n",
    "\n",
    "# Plot the training and testing accuracies\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('KNN: Varying Number of Neighbors')\n",
    "plt.plot(neighbors, my_train, label='Training Accuracy')\n",
    "plt.plot(neighbors, my_test, label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay781ls2p6s8"
   },
   "source": [
    "## regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y29iTY0hp-Z2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "sales_df = pd.read_csv('sales_df.csv')\n",
    "\n",
    "# Step 1: Create X and y\n",
    "X = sales_df['radio'].values\n",
    "y = sales_df['sales'].values\n",
    "\n",
    "# Reshape X into a two-dimensional array\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# Print the shape of X and y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Step 2: Build a linear regression model\n",
    "# Instantiate a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict sales values\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print the first five prediction values\n",
    "print(\"First five predictions:\", predictions[:5])\n",
    "\n",
    "# Step 3: Visualize the linear regression model\n",
    "# Create a scatter plot of y against X\n",
    "plt.scatter(X, y, color='blue', label='Observations')\n",
    "\n",
    "# Plot the predictions against X\n",
    "plt.plot(X, predictions, color='red', label='Linear Regression')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Radio Advertising Expenditure')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtwznWNtrMRe"
   },
   "source": [
    "## fitting and predicting with a multiple linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiJgp9FlrLYe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Load the dataset\n",
    "sales_df = pd.read_csv('sales_df.csv')\n",
    "\n",
    "# Step 1: Create X and y\n",
    "X = sales_df.drop(\"sales\", axis=1).values\n",
    "y = sales_df[\"sales\"].values\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Build a linear regression model\n",
    "# Instantiate a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predict sales values using the test features\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the first two values of y_pred and y_test\n",
    "print(\"First two predictions:\", y_pred[:2])\n",
    "print(\"First two actual values:\", y_test[:2])\n",
    "\n",
    "# Step 5: Calculate the model's R-squared score\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "# Step 6: Calculate the model's root mean squared error\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Print R-squared and RMSE\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcnAfJYTrM-m"
   },
   "source": [
    "## performing 6-fold cross-validation with a linear regression model\n",
    "and displaying individual scores, mean, standard deviation, and the 95% confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5tkOUuBrL95"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "sales_df = pd.read_csv('sales_df.csv')\n",
    "\n",
    "# Create X and y\n",
    "X = sales_df[['radio', 'social_media']].values\n",
    "y = sales_df['sales'].values\n",
    "\n",
    "# Create KFold object with 6 splits, shuffle=True, and random seed 5\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=kf)\n",
    "\n",
    "# Print individual scores\n",
    "print(\"Individual CV Scores:\", cv_scores)\n",
    "\n",
    "# Calculate and print the mean of the CV scores\n",
    "mean_score = np.mean(cv_scores)\n",
    "print(\"Mean CV Score:\", mean_score)\n",
    "\n",
    "# Calculate and print the standard deviation of the CV scores\n",
    "std_deviation = np.std(cv_scores)\n",
    "print(\"Standard Deviation of CV Scores:\", std_deviation)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "lower_bound = np.quantile(cv_scores, 0.025)\n",
    "upper_bound = np.quantile(cv_scores, 0.975)\n",
    "print(\"95% Confidence Interval:\", (lower_bound, upper_bound))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gH39v7cmsHqR"
   },
   "source": [
    "## Lasso regression and visualizing the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Yyr5LaYsHeg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_df = pd.read_csv('diabetes.csv', index_col=0)\n",
    "\n",
    "# Filter out rows with zero BMI and glucose values\n",
    "diabetes_df = diabetes_df[(diabetes_df['bmi'] != 0) & (diabetes_df['glucose'] != 0)]\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = diabetes_df.drop('glucose', axis=1).values\n",
    "y = diabetes_df['glucose'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Get the feature names\n",
    "names = diabetes_df.drop('glucose', axis=1).columns\n",
    "\n",
    "# Create a Lasso regression model with alpha = 0.1\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the Lasso model on the data and get the coefficients\n",
    "lasso_coef = lasso.fit(X, y).coef_\n",
    "\n",
    "# Create a bar plot to visualize the coefficients\n",
    "plt.bar(names, lasso_coef)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Lasso Coefficients\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLIR4aXsVNl4"
   },
   "source": [
    "# W10 - Machine Learning with Scikit-learn part 2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6RqxksWtCDY"
   },
   "source": [
    "## All from chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oePbvdAfVHeJ"
   },
   "outputs": [],
   "source": [
    "# Classification metrics\n",
    "# Measuring model performance with accuracy: Fraction of correctly classified samples\n",
    "# Not always a useful metric\n",
    "\n",
    "# Class imbalance\n",
    "# Classification for predicting fraudulent bank transactions\n",
    "# 99% of transactions are legitimate; 1% are fraudulent\n",
    "# Could build a classifier that predicts NONE of the transactions are fraudulent\n",
    "# 99% accurate!\n",
    "# But terrible at actually predicting fraudulent transactions\n",
    "# Fails at its original purpose\n",
    "# Class imbalance: Uneven frequency of classes\n",
    "# Need a different way to assess performance\n",
    "\n",
    "# The false negatives are the number of legitimate transactions incorrectly labeled\n",
    "# The false positives are the number of transactions incorrectly labeled as fraudulent.\n",
    "# The true negatives are the number of legitimate transactions correctly labeled\n",
    "# The true positives are the number of fraudulent transactions correctly labeled\n",
    "\n",
    "# Firstly, we can retrieve accuracy: it's the sum of true predictions divided by the total sum of the matrix.\n",
    "\n",
    "# Usually, the class of interest is called the positive class. As we aim to detect fraud, the positive class is an illegitimate transaction. So why is the confusion matrix important? There are other important metrics we can calculate from the confusion matrix. Precision is the number of true positives divided by the sum of all positive predictions. It is also called the positive predictive value. In our case, this is the number of correctly labeled fraudulent transactions divided by the total number of transactions classified as fraudulent. High precision means having a lower false positive rate.\n",
    "\n",
    "# Recall is the number of true positives divided by the sum of true positives and false negatives. This is also called sensitivity. High recall reflects a lower false negative rate. For our classifier, it means predicting most fraudulent transactions correctly.\n",
    "\n",
    "# The F1-score is the harmonic mean of precision and recall. This metric gives equal weight to precision and recall, therefore it factors in both the number of errors made by the model and the type of errors. The F1 score favors models with similar precision and recall, and is a useful metric if we are seeking a model which performs reasonably well across both metrics.\n",
    "\n",
    "# This report includes precision and recall by class (churn: 1 or no churn: 0), point-seven-six and point-one-six for the churn class respectively, which highlights how poorly the model's recall is on the churn class. Support represents the number of instances for each class within the true labels. (Class imbalance)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "# Lasso regression: choosing alpha\n",
    "# KNN: choosing n_neighbors\n",
    "# Hyperparameters: parameters we specify before fitting the model\n",
    "\n",
    "# We perform k-fold cross-validation for each combination of hyperparameters. The mean scores for each combination are shown here. By default, KNN uses Euclidean distance but Manhattan distance can be selected by setting p = 1.\n",
    "# For example, .KNeighborsClassifier(n_neighbors=5, p=1)\n",
    "\n",
    "# Preprocessing Data\n",
    "\n",
    "# We will be working with a music dataset in this chapter, for both classification and regression problems. Initially, we will build a regression model using all features in the dataset to predict song popularity. There is one categorical feature, genre, with ten possible values.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "music_df = pd.read_csv('music.csv', index_col=0)\n",
    "music_dummies = pd.get_dummies(music_df['genre'], drop_first=True)\n",
    "music_dummies = pd.concat([music_df, music_dummies], axis=1)\n",
    "music_dummies = music_dummies.drop('genre', axis=1)\n",
    "\n",
    "X = music_dummies.drop('popularity', axis=1).values\n",
    "y = music_dummies['popularity'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "linreg = LinearRegression()\n",
    "linreg_cv = cross_val_score(linreg, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "linreg_cv2 = cross_val_score(linreg, X_train, y_train, cv=kf)\n",
    "print(np.sqrt(-linreg_cv))\n",
    "print(linreg_cv2)\n",
    "\n",
    "music_df = pd.read_csv('music_unclean.csv', index_col=0)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "music_df = pd.read_csv('music_unclean.csv', index_col=0)\n",
    "music_df = music_df.dropna(subset=['genre', 'popularity', 'loudness', 'liveness', 'tempo'])\n",
    "music_df['genre'] = np.where(music_df['genre'] == 'Rock', 1, 0)\n",
    "\n",
    "X = music_df.drop('genre', axis=1).values\n",
    "y = music_df['genre'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "steps = [('imputation', SimpleImputer()), ('Log_reg', LogisticRegression())]\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(pipeline.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CA1E4rTJs2WJ"
   },
   "source": [
    "## splits it into training and testing sets, performs cross-validation with linear regression, and prints the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-z7LMPYs2od"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the music dataset\n",
    "music_df = pd.read_csv('music.csv', index_col=0)\n",
    "\n",
    "# Create dummy variables for the 'genre' column\n",
    "music_dummies = pd.get_dummies(music_df['genre'], drop_first=True)\n",
    "\n",
    "# Combine the original dataset with dummy variables\n",
    "music_dummies = pd.concat([music_df, music_dummies], axis=1)\n",
    "\n",
    "# Drop the original 'genre' column\n",
    "music_dummies = music_dummies.drop('genre', axis=1)\n",
    "\n",
    "# Print the columns of the resulting dataframe\n",
    "print(music_dummies.columns)\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = music_dummies.drop('popularity', axis=1).values\n",
    "y = music_dummies['popularity'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize KFold with 5 splits and shuffle\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Perform cross-validation for negative mean squared error (MSE)\n",
    "linreg_cv = cross_val_score(linreg, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform cross-validation without specifying a scoring method\n",
    "linreg_cv2 = cross_val_score(linreg, X_train, y_train, cv=kf)\n",
    "\n",
    "# Calculate and print the RMSE from the negative MSE values\n",
    "print(np.sqrt(-linreg_cv))\n",
    "\n",
    "# Print cross-validation scores without specifying a scoring method\n",
    "print(linreg_cv2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PFiujoXtnNo"
   },
   "source": [
    "## preprocesses the music dataset, handles missing values, encodes the 'genre' column, and uses a pipeline to perform imputation and classification with k-nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qE4_NsRtnr9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the uncleaned music dataset\n",
    "music_df = pd.read_csv(\"music_unclean.csv\")\n",
    "\n",
    "# Print the number of missing values in each column (ascending order)\n",
    "print(music_df.isna().sum().sort_values(ascending=True))\n",
    "\n",
    "# Drop rows with missing values in specific columns\n",
    "music_df = music_df.dropna(subset=['genre', 'popularity', 'loudness', 'liveness', 'tempo'])\n",
    "\n",
    "# Encode 'genre' column as binary (Rock vs. non-Rock)\n",
    "music_df['genre'] = np.where(music_df['genre'] == 'Rock', 1, 0)\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = music_df.drop('genre', axis=1).values\n",
    "y = music_df['genre'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize SimpleImputer for missing value imputation\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Initialize KNeighborsClassifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Create a pipeline with imputer and KNeighborsClassifier\n",
    "steps = [(\"imputer\", imputer), (\"knn\", knn)]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print confusion matrix and accuracy score\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "go18Lzu_tpOY"
   },
   "source": [
    "## preprocesses the music dataset, handles missing values, encodes the 'genre' column, and uses k-nearest neighbors (KNN) for classification without a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zp13tdR-uElM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the uncleaned music dataset\n",
    "music_df = pd.read_csv(\"music_unclean.csv\")\n",
    "\n",
    "# Print the number of missing values in each column (ascending order)\n",
    "print(music_df.isna().sum().sort_values(ascending=True))\n",
    "\n",
    "# Drop rows with missing values in specific columns\n",
    "music_df = music_df.dropna(subset=['genre', 'popularity', 'loudness', 'liveness', 'tempo'])\n",
    "\n",
    "# Encode 'genre' column as binary (Rock vs. non-Rock)\n",
    "music_df['genre'] = np.where(music_df['genre'] == 'Rock', 1, 0)\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = music_df.drop('genre', axis=1).values\n",
    "y = music_df['genre'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize SimpleImputer for missing value imputation with strategy 'mean'\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.transform(X_test)\n",
    "\n",
    "# Define the feature column names (excluding 'genre')\n",
    "columns = ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']\n",
    "\n",
    "# Create a DataFrame with imputed values and column names\n",
    "check = pd.DataFrame(X_train, columns=columns)\n",
    "\n",
    "# Initialize KNeighborsClassifier with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy score\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8YAgmjk_-EO"
   },
   "source": [
    "upper syntax seen be error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "D9VvhAgxADr9",
    "outputId": "21f9fb5a-f68d-4a66-ea0e-3512a4020cb9"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dad1a697185b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the uncleaned music dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmusic_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"music_unclean.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Print the number of missing values in each column (ascending order)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'music_unclean.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the uncleaned music dataset\n",
    "music_df = pd.read_csv(\"music_unclean.csv\")\n",
    "\n",
    "# Print the number of missing values in each column (ascending order)\n",
    "print(music_df.isna().sum().sort_values(ascending=True))\n",
    "\n",
    "# Drop rows with missing values in specific columns\n",
    "music_df = music_df.dropna(subset=['genre', 'popularity', 'loudness', 'liveness', 'tempo'])\n",
    "\n",
    "# Encode 'genre' column as binary (Rock vs. non-Rock)\n",
    "music_df['genre'] = np.where(music_df['genre'] == 'Rock', 1, 0)\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = music_df.drop('genre', axis=1).values\n",
    "y = music_df['genre'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize SimpleImputer for missing value imputation with strategy 'mean'\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.transform(X_test)\n",
    "\n",
    "# Define the feature column names (excluding 'genre')\n",
    "columns = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']\n",
    "\n",
    "# Initialize KNeighborsClassifier with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy score\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SqW5N_EuTON"
   },
   "source": [
    "# W11 - Credit card approval predictor.PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZPg1rcc6TU5"
   },
   "source": [
    "## Supervised Machine learning by LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIzjowlJ51K_"
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read the credit card approvals dataset\n",
    "cc_apps = pd.read_csv(\"cc_approvals.data\", header=None)\n",
    "cc_apps.head()\n",
    "\n",
    "# Display a summary of the dataset\n",
    "print(cc_apps.describe())\n",
    "print('\\n')\n",
    "print(cc_apps.info())\n",
    "print('\\n')\n",
    "\n",
    "# Display the last 17 rows of the dataset\n",
    "cc_apps.tail(17)  # or cc_apps.sample()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "print(cc_apps.corr())\n",
    "\n",
    "# Drop the features 11 and 13\n",
    "cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "cc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Replace the '?'s with NaN in the train and test sets\n",
    "cc_apps_train = cc_apps_train.replace('?', np.NaN)\n",
    "cc_apps_test = cc_apps_test.replace('?', np.NaN)\n",
    "\n",
    "# Impute the missing values with mean imputation\n",
    "cc_apps_train.fillna(cc_apps_train.mean(), inplace=True)\n",
    "cc_apps_test.fillna(cc_apps_train.mean(), inplace=True)\n",
    "\n",
    "# Count the number of NaNs in the datasets and print the counts to verify\n",
    "print(cc_apps_train.isnull().sum())\n",
    "print(cc_apps_test.isnull().sum())\n",
    "\n",
    "for col in cc_apps_train.columns:\n",
    "    # Iterate over each column of cc_apps_train\n",
    "    if cc_apps_train[col].dtypes == 'object':\n",
    "        # Check if the column is of object type\n",
    "        # Impute with the most frequent value\n",
    "        # The value_counts() function returns a Series that contains counts of unique values.\n",
    "        # It returns an object that will be in descending order, so the first element will be the most frequently-occurred element.\n",
    "        cc_apps_train = cc_apps_train.fillna(cc_apps_train[col].value_counts().index[0])\n",
    "        cc_apps_test = cc_apps_test.fillna(cc_apps_train[col].value_counts().index[0])\n",
    "\n",
    "# Count the number of NaNs in the dataset and print the counts to verify\n",
    "print(cc_apps_train.isnull().sum())\n",
    "print(cc_apps_test.isnull().sum())\n",
    "\n",
    "# At this point, there are no missing values.\n",
    "\n",
    "# Convert the categorical features in the train and test sets independently\n",
    "print(cc_apps_train)\n",
    "cc_apps_train = pd.get_dummies(cc_apps_train)\n",
    "cc_apps_test = pd.get_dummies(cc_apps_test)\n",
    "print(cc_apps_train)\n",
    "\n",
    "# Reindex the columns of the test set aligning with the train set\n",
    "cc_apps_test = cc_apps_test.reindex(columns=cc_apps_train.columns, fill_value=0)\n",
    "\n",
    "# Import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Segregate features and labels into separate variables\n",
    "# .ravel() ensure that y_train and y_test are 1D arrays as expected by the Logistic Regression model.\n",
    "X_train, y_train = cc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values.ravel()\n",
    "X_test, y_test = cc_apps_test.iloc[:, :-1].values, cc_apps_test.iloc[:, [-1]].values.ravel()\n",
    "\n",
    "# Instantiate MinMaxScaler and use it to rescale X_train and X_test\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.transform(X_test)\n",
    "\n",
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate a LogisticRegression classifier with default parameter values\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit logreg to the train set\n",
    "logreg.fit(rescaledX_train, y_train)\n",
    "\n",
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use logreg to predict instances from the test set and store it\n",
    "y_pred = logreg.predict(rescaledX_test)\n",
    "\n",
    "# Get the accuracy score of the logreg model and print it\n",
    "print(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test, y_test))\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTPvBn9_7p_-"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VhTXeZI7uWZ"
   },
   "outputs": [],
   "source": [
    "# Import pandas and other libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Read the credit card approvals dataset\n",
    "cc_apps = pd.read_csv(\"cc_approvals.data\", header=None)\n",
    "\n",
    "# Display a summary of the dataset\n",
    "print(cc_apps.describe())\n",
    "print('\\n')\n",
    "print(cc_apps.info())\n",
    "print('\\n')\n",
    "\n",
    "# Drop the features 11 and 13\n",
    "cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "cc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\n",
    "\n",
    "# Replace the '?'s with NaN in the train and test sets\n",
    "cc_apps_train = cc_apps_train.replace('?', np.NaN)\n",
    "cc_apps_test = cc_apps_test.replace('?', np.NaN)\n",
    "\n",
    "# Impute the missing values with mean imputation\n",
    "cc_apps_train.fillna(cc_apps_train.mean(), inplace=True)\n",
    "cc_apps_test.fillna(cc_apps_train.mean(), inplace=True)\n",
    "\n",
    "# Convert the categorical features in the train and test sets independently\n",
    "cc_apps_train = pd.get_dummies(cc_apps_train)\n",
    "cc_apps_test = pd.get_dummies(cc_apps_test)\n",
    "\n",
    "# Reindex the columns of the test set aligning with the train set\n",
    "cc_apps_test = cc_apps_test.reindex(columns=cc_apps_train.columns, fill_value=0)\n",
    "\n",
    "# Segregate features and labels into separate variables\n",
    "X_train, y_train = cc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values.ravel()\n",
    "X_test, y_test = cc_apps_test.iloc[:, :-1].values, cc_apps_test.iloc[:, [-1]].values.ravel()\n",
    "\n",
    "# Instantiate MinMaxScaler and use it to rescale X_train and X_test\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.transform(X_test)\n",
    "\n",
    "# Find the best 'k' for KNN using cross-validation\n",
    "best_k = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(1, 21):  # You can adjust the range of k as needed\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, rescaledX_train, y_train, cv=5)\n",
    "    mean_accuracy = scores.mean()\n",
    "\n",
    "    if mean_accuracy > best_accuracy:\n",
    "        best_accuracy = mean_accuracy\n",
    "        best_k = k\n",
    "\n",
    "# Train KNN with the best 'k' on the entire training set\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "best_knn.fit(rescaledX_train, y_train)\n",
    "\n",
    "# Use the trained KNN to make predictions on the test set\n",
    "y_pred = best_knn.predict(rescaledX_test)\n",
    "\n",
    "# Get the accuracy score of the KNN model and print it\n",
    "accuracy = best_knn.score(rescaledX_test, y_test)\n",
    "print(f\"Accuracy of KNN classifier with k={best_k}: {accuracy}\")\n",
    "\n",
    "# Print the confusion matrix of the KNN model\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsV9Oiqv70-m"
   },
   "source": [
    "data is split into train and test sets before imputing and preprocessing.\n",
    "Your task here is to\n",
    "## Inverse these steps by performing cleaning, imputing and preprocessing before splitting the data into train and test sets.\n",
    " Use a Logistic Regression and evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7-4j1ln8RZn"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Read the credit card approvals dataset\n",
    "cc_apps = pd.read_csv(\"cc_approvals.data\", header=None)\n",
    "\n",
    "# Display a summary of the dataset\n",
    "print(cc_apps.describe())\n",
    "print('\\n')\n",
    "print(cc_apps.info())\n",
    "print('\\n')\n",
    "\n",
    "# Drop the features 11 and 13\n",
    "cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "\n",
    "# Replace the '?'s with NaN\n",
    "cc_apps = cc_apps.replace('?', np.NaN)\n",
    "\n",
    "# Impute the missing values with mean imputation\n",
    "cc_apps.fillna(cc_apps.mean(), inplace=True)\n",
    "\n",
    "# Convert the categorical features\n",
    "cc_apps = pd.get_dummies(cc_apps)\n",
    "\n",
    "# Reindex the columns of the dataset to align with the train set\n",
    "cc_apps = cc_apps.reindex(columns=cc_apps.columns, fill_value=0)\n",
    "\n",
    "# Segregate features and labels into separate variables\n",
    "X = cc_apps.iloc[:, :-1].values\n",
    "y = cc_apps.iloc[:, -1].values\n",
    "\n",
    "# Instantiate MinMaxScaler and use it to rescale X\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(rescaledX, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Instantiate a LogisticRegression classifier with default parameter values\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit logreg to the train set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Use logreg to predict instances from the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Get the accuracy score of the logreg model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of logistic regression classifier: {accuracy}\")\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
